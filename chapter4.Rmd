# Chapter 4: Clustering and Classification


<<<<<<< HEAD
## Part 1 and 2: Creating and  exploring the datafile 
=======
## Part 1 and 2: Creating and exploring the datafile 
>>>>>>> 6263dd31d375ba85afa8254fee3029054a11ba01

 

```{r}

# access the MASS package and other packages
library(MASS)

library(tidyr); library(dplyr); library(ggplot2)
library(readr)

# load the data
data("Boston")

# explore the dataset
str(Boston)
dim(Boston)
colnames(Boston)

# The Boston data includes different variables to analyze housing Values in the suburbs of Boston. These variables include crime rates, average number of rooms, distance form Boston employment centers, and accessibility to highways.The data set contains 506 observations from 14 variables. Most of the variables are numeric, but there are also some integers.


```

## Part 3: A graphical overview of the data and summaries of the variables 



```{r}

library(ggplot2)
library(tidyverse)
library(readr)
library(dplyr)
library(GGally)

# Plottings graphs of variables
ggplot(Boston, aes(x = tax, y = medv)) + geom_point() + geom_smooth(method = "lm") + ggtitle("Housing values and property taxes")

ggplot(Boston, aes(x = dis, y = medv)) + geom_point() + geom_smooth(method = "lm") + ggtitle("Housing values and distance from employment centers")

ggplot(Boston, aes(x = ptratio, y = medv)) + geom_point() + geom_smooth(method = "lm") + ggtitle("Housing values and pupil-teather ratio")

ggplot(Boston, aes(x = lstat, y = medv)) + geom_point() + geom_smooth(method = "lm") + facet_wrap("chas") + ggtitle("Housing values and lower status families per closeness to River")

# The graphs showcase some examples of the relationships between housing values and the other variables. For example, there seems to be a positive relation between housing values and distance to employment centers and a negative relation between housing values and pupil-teacher ratio. Both relations seem fairly intuitive.


# Plotting a matrix with ggpairs()
pairs(Boston[-1])
p <- ggpairs(Boston, mapping = aes(), lower = list(combo = wrap("facethist", bins = 10)))

library(corrplot)
cor_matrix <- cor(Boston) 
cor_matrix
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d")

# The correlation plot shows a visual representation of correlations between the variables, with bigger bubble and a darker indicating high correlation in absolute terms. Also, the correlation matrix shows correlations of the different variables.

# Showing summary of the variables
summary(Boston)

# Summary statistics show for each variables min and max values,mean and median values as well as the 1st and 3rd quantiles. One can observe that the variables have quite different scales and characteristics.

```

## Part 4: Standardizing the dataset, creating a new crime rate variable and dividing the data set to train and test sets


```{r}

# Scaling the Boston dataset
boston_scaled <- scale(Boston)

# Summaries of the scaled variables
summary(boston_scaled)

# Changing the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

#Standardization centers each variable around mean = 0 and scales the variables by their standard deviation. This makes them more easily comparable. 


#Creating a categorical variable of the crime rate in the Boston dataset 

boston_scaled$crim <- as.numeric(boston_scaled$crim)

# looking at a summary of the scaled crime rate
summary(boston_scaled$crim)

# Creating a quantile vector of crim and printing it
bins <- quantile(boston_scaled$crim)
bins

# Creating a categorical variable 'crime' by using the quantiles as the break points in the categorical variable 
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, levels = c("low", "med_low", "med_high", "high"))

# Looking at the table of the new factor crime
table(crime)

# Removing original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# Adding the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)


# Dividing the data set to train and test sets

# Looking at the number of rows in the scaled Boston dataset 
n <- nrow(boston_scaled)

# Choosing randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# Creating the train set that includes the randomly chosen 80 percent of rows
train <- boston_scaled[ind,]

# Creating the test set that includes the rest of the rows
test <- boston_scaled[-ind,]

# Saving the correct classes from test data
correct_classes <- test$crime

# Removing the crime variable from test data
test <- dplyr::select(test, -crime)



```


## Part 5 and 6: Linear discriminant analysis 

Linear discriminant analysis is a classification method that finds the (linear) combination of the variables that separate the target variable classes. 


```{r}

# Fitting the linear discriminant analysis. The categorical crime rate is the target variable and all the other variables are the predictor variables
lda.fit <- lda(crime ~ ., data = train)

# Printing the lda.fit object
lda.fit

# Creating a lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# Target classes as numeric
classes <- as.numeric(train$crime)

# Plotting the lda results 
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

# Predicting classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# Cross-tabulating the results
table(correct = correct_classes, predicted = lda.pred$class)

# The result shows that 74 of 102, meaning about 71 %, are predicted at the correct class (indicated by the range of the quantile)

```



## Part 7: Clustering and distance measures  



```{r}

# Reloading and scaling the Boston data set
library(MASS)
data("Boston")
boston_scaled <- as.data.frame(scale(Boston))
boston_scaled$crim <- as.numeric(boston_scaled$crim)

# Calculating the distances between the observations using the Euclidean method
dist_eu <- dist(boston_scaled, method = "euclidean")

# Looking at the summary of the distances
summary(dist_eu)


library(ggplot2)
set.seed(123)
library(tidyverse) 
library(cluster) 

# Running k-means algorithm and investigate the optimal number of clusters

# Looking first at k-means clustering with 3 centers
km <- kmeans(boston_scaled, centers = 3)

# Plotting the Boston dataset with clusters for selected variables
pairs(boston_scaled[c("rm", "age", "dis", "crim")], col = km$cluster)


# Now, trying to find the optimal number of clusters

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})


# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# The optimal number of clusters is when the value of total WCSS changes radically. The graphs shows that this happens with two clusters.  

# k-means clustering with the optimal number of clusters
km <- kmeans(boston_scaled, centers = 2)

# Plotting the Boston dataset with clusters (looking at only selected variables for easier visualization)
pairs(boston_scaled[c("rm", "age", "dis", "crim")], col = km$cluster)

# The plots in red and black the separation of the data points to two different clusters. For some selected variables chosen, it shows that quite clear clustering around certain values. 

```



```{r}
# End of Chapter 4
date()


```
